{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b08ddab1",
   "metadata": {},
   "source": [
    "## Q1. How does bagging reduce overfitting in decision trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68137d88",
   "metadata": {},
   "source": [
    "It works by creating multiple subsets of the training data by sampling with replacement (bootstrap sampling) and then training a separate model on each subset. This helps to reduce overfitting and improve generalization by averaging out the biases and variances of individual models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f72ae2",
   "metadata": {},
   "source": [
    "## Q2. What are the advantages and disadvantages of using different types of base learners in bagging?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68b9716",
   "metadata": {},
   "source": [
    "Advantages\n",
    "1. It can include more diversity in learning, less chances of overfitting and improves performance.\n",
    "\n",
    "Disadvantages\n",
    "1. It is more like a black box, difficult to interpret."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896689f9",
   "metadata": {},
   "source": [
    "## Q3. How does the choice of base learner affect the bias-variance tradeoff in bagging?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af82bbdc",
   "metadata": {},
   "source": [
    "In bagging (Bootstrap Aggregating), the choice of base learner can indeed influence the bias-variance tradeoff. Here's how:\n",
    "\n",
    "Low Bias, High Variance Base Learners: If you use base learners with low bias but high variance (such as decision trees), bagging can significantly reduce the variance. This is because bagging relies on averaging the predictions of multiple models trained on different bootstrap samples of the data. By doing so, it smooths out the high variance in individual predictions, leading to a reduction in the overall variance without significantly affecting bias.\n",
    "\n",
    "High Bias, Low Variance Base Learners: When base learners have high bias but low variance (like linear models), bagging may not improve the bias much. However, it can still reduce variance by introducing diversity among the models. Since each model in the ensemble is trained on a different subset of the data, they make different errors, which when averaged out, reduces the overall variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad406cc",
   "metadata": {},
   "source": [
    "## Q4. Can bagging be used for both classification and regression tasks? How does it differ in each case?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4802b662",
   "metadata": {},
   "source": [
    "yes, bagging is a versatile  ensemble technique that can be applied to both classification and regression tasks. However, the method of aggregating predictions differs slightly between the two:\n",
    "\n",
    "Classification:\n",
    "\n",
    "Prediction Aggregation: In classification, the final prediction is typically made by majority voting. Each base model in the ensemble casts a \"vote\" for the most likely class. The class with the highest number of votes becomes the final prediction for the new data point.\n",
    "Regression:\n",
    "\n",
    "Prediction Aggregation: For regression tasks, the bagging approach averages the predictions from all the base models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975df8d1",
   "metadata": {},
   "source": [
    "## Q5. What is the role of ensemble size in bagging? How many models should be included in the ensemble?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726bef42",
   "metadata": {},
   "source": [
    "Impact of Ensemble Size: The number of base learners in the ensemble also affects the bias-variance tradeoff. As you increase the number of base learners, the bias of the ensemble typically remains unchanged (or decreases slightly), while the variance continues to decrease. However, there's a point of diminishing returns, where adding more base learners may not lead to significant improvements in the variance reduction but can increase computational costs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ae723a",
   "metadata": {},
   "source": [
    "## Q6. Can you provide an example of a real-world application of bagging in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c1213f",
   "metadata": {},
   "source": [
    "Financial Fraud Detection, AIP Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf144ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
