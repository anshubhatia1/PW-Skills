{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7be1d49f",
   "metadata": {},
   "source": [
    "### Q1. What is Ridge Regression, and how does it differ from ordinary least squares regression?\n",
    "\n",
    "\n",
    "It is also called as L2 regularization, It is added to make sure that first cost function is not zero and also to reduce the magnitude of coefficients so that we can control overfitting by addding a penality term.\n",
    "\n",
    "OLS: OLS tends to have low bias but can have high variance, especially when the number of predictors is large relative to the number of observations.\n",
    "\n",
    "Ridge Regression: By introducing the penalty term, ridge regression increases the bias slightly but reduces the variance of the model. This trade-off often leads to improved predictive performance on new, unseen data.\n",
    "\n",
    "Ridge regression is a regularization technique that modifies ordinary least squares regression by adding a penalty term to the objective function. This penalty term improves the stability of the model by reducing the variance of the coefficient estimates, handling multicollinearity, and providing a solution in cases where ordinary least squares may fail. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f85f3db",
   "metadata": {},
   "source": [
    "### Q2. What are the assumptions of Ridge Regression?\n",
    "\n",
    "The assumptions of Ridge Regression are:\n",
    "\n",
    "1. **Linearity:** The relationship between the independent variables and the dependent variable should be linear.\n",
    "   \n",
    "2. **No Perfect Multicollinearity:** The independent variables should not be perfectly correlated with each other.\n",
    "\n",
    "3. **Independence of Errors:** The errors (residuals) should be independent of each other.\n",
    "\n",
    "4. **Homoscedasticity:** The variance of the errors should be constant across all levels of the independent variables.\n",
    "\n",
    "5. **Normality of Errors:** The errors should follow a normal distribution.\n",
    "\n",
    "Ridge Regression is a regularized extension of linear regression that addresses multicollinearity and helps improve the stability of coefficient estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bdbd53",
   "metadata": {},
   "source": [
    "### Q4. Can Ridge Regression be used for feature selection? If yes, how?\n",
    "\n",
    "We can use it for feature selection, but it wont be as aggresive as lasso in case of feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99436e7e",
   "metadata": {},
   "source": [
    "### Q5. How does the Ridge Regression model perform in the presence of multicollinearity?\n",
    " Ridge Regression is robust in the presence of multicollinearity due to its ability to stabilize coefficient estimates and control overfitting through regularization. It is particularly useful when dealing with datasets where predictors are highly correlated, providing more reliable and generalized predictions compared to ordinary least squares regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74405884",
   "metadata": {},
   "source": [
    "### Q6. Can Ridge Regression handle both categorical and continuous independent variables?\n",
    "\n",
    "Ridge Regression is flexible and can handle both categorical and continuous variables in regression models. However, categorical variables need to be appropriately encoded to numeric form before fitting the model. This flexibility makes Ridge Regression applicable to a wide range of datasets containing mixed types of independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a16da2",
   "metadata": {},
   "source": [
    "### Q7. How do you interpret the coefficients of Ridge Regression?\n",
    "For instance, if you have a Ridge Regression model predicting house prices with predictors like square footage, number of bedrooms, and neighborhood indicators:\n",
    "\n",
    "A coefficient of 0.2 for square footage would imply that, all else being equal, a one-unit increase in square footage is associated with a 0.2-unit increase in house price.\n",
    "A negative coefficient of -0.1 for a neighborhood indicator might indicate that houses in that neighborhood tend to have lower prices compared to the reference neighborhood.\n",
    "In summary, while Ridge Regression modifies the interpretation of coefficients due to its regularization effect, the basic principles of coefficient interpretation remain intact, focusing on magnitude, direction, and relative importance of predictors in relation to the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00514360",
   "metadata": {},
   "source": [
    "### Q8. Can Ridge Regression be used for time-series data analysis? If yes, how?\n",
    "\n",
    "\n",
    "Yes, Ridge Regression can be used for time-series data analysis by treating lagged values of the target variable and other relevant predictors as features. The regularization parameter \n",
    "ùúÜ\n",
    "Œª helps control overfitting, and cross-validation can be used to select an optimal \n",
    "ùúÜ\n",
    "Œª value. However, Ridge Regression does not explicitly model time dependencies, so careful feature engineering with lagged variables is essential to capture temporal patterns effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba172e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
